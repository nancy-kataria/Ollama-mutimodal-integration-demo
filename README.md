# Ollama-multimodal-integration-demo
Integrating text and vision models using Ollama, supporting both text and image-based AI interactions.

Models: gemma3:4b and llava:7b 

## How to use the model

1. Create a virtual environment.
 ```
 python -m venv .venv
 ```
2. Activate the virtual environment.
```
source .venv/bin/activate
```
3. Install the requirements.
```
pip install -r requirements.txt
```
4. Download [Ollama](https://ollama.com/) on your system.
5. Run the scripts.
```
python gemma-text.py
```
```
python llava_vision.py
```
